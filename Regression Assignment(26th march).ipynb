{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e836414",
   "metadata": {},
   "source": [
    "1) Simple linear regression has one X and Y variable. Multiple linear regression has one y and two aor more X variables.For example. if we calculate the price of the house on the basis of only square feet then it is simple linear regression, if we calculate the price of the house on the basis of square feet and age of the building then it is multiple linear regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9477f74",
   "metadata": {},
   "source": [
    "2) The assumptions are:\n",
    "a) There is a linear relationship between the predictors(x) and the outcome(y).\n",
    "b) Predictors (x) are independent and observed with negligible error.\n",
    "c) Residual errors have a mean value of zero.\n",
    "d) Residual Errors have constant variance.\n",
    "e) Residual errors are independent from each other and predictors(x).\n",
    "Non-linearity is usually most evident in a plot of observed versus predicted values or a plot of residuals versus predicted values, which are a part of standard regression output.The points should be symmetrically distributed around a diagonal line in the former plot or around horizontal line in the latter plot, with a roughly constant variance.In multiple regression models, nonlinearity or nonadditivity may also be reveleaded by systematic patterns in plots of the residuals versus individual indepenedent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc2e36a",
   "metadata": {},
   "source": [
    "3) In a linear regression model, the slope represents the change in the dependent variable (y) for a one-unit change in the independent variable (x), while the intercept represents the value of y when x is zero. \n",
    "\n",
    "For example, let's say you're analyzing the relationship between the number of hours studied (x) and exam score (y). If the slope of the regression line is 0.5, it means that for every additional hour studied, the exam score is expected to increase by 0.5 points. If the intercept is 60, it means that a student who studied zero hours is expected to score 60 points on the exam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6998302",
   "metadata": {},
   "source": [
    "4) Gradient descent is an optimization algorithm used in machine learning to minimize the loss function of a model by iteratively adjusting its parameters. \n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1. *Initialization*: Start with initial values for the model's parameters (weights and biases).\n",
    "\n",
    "2. *Calculate Gradient*: Compute the gradient of the loss function with respect to each parameter. The gradient indicates the direction of the steepest increase in the loss function.\n",
    "\n",
    "3. *Update Parameters*: Adjust the parameters in the opposite direction of the gradient to minimize the loss function. This adjustment is done by subtracting a fraction of the gradient from the current parameter values, scaled by a learning rate (step size).\n",
    "\n",
    "4. *Iterate*: Repeat steps 2 and 3 until the algorithm converges to a minimum of the loss function, or until a predefined number of iterations is reached.\n",
    "\n",
    "Gradient descent is used in various machine learning algorithms, including linear regression, logistic regression, neural networks, and support vector machines, to find the optimal parameters that minimize the difference between predicted and actual outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0d07f4",
   "metadata": {},
   "source": [
    "5) Multiple linear regression is an extension of simple linear regression that allows for more than one independent variable to predict a single dependent variable. In multiple linear regression, the relationship between the dependent variable and multiple independent variables is modeled as a linear equation:\n",
    "\n",
    "\\[ y = b_0 + b_1x_1 + b_2x_2 + \\ldots + b_nx_n + \\epsilon \\]\n",
    "\n",
    "where:\n",
    "- \\( y \\) is the dependent variable.\n",
    "- \\( x_1, x_2, \\ldots, x_n \\) are the independent variables.\n",
    "- \\( b_0 \\) is the intercept (the value of \\( y \\) when all independent variables are zero).\n",
    "- \\( b_1, b_2, \\ldots, b_n \\) are the coefficients (slopes) associated with each independent variable, indicating the change in \\( y \\) for a one-unit change in the corresponding independent variable.\n",
    "- \\( \\epsilon \\) represents the error term.\n",
    "\n",
    "Multiple linear regression differs from simple linear regression in that it considers the influence of multiple independent variables on the dependent variable, allowing for a more nuanced understanding of the relationship between them. While simple linear regression involves only one independent variable, multiple linear regression can capture more complex relationships and provide insights into how different factors collectively affect the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912509a5",
   "metadata": {},
   "source": [
    "6) Multicollinearity occurs in multiple linear regression when two or more independent variables are highly correlated with each other. This can cause issues in the regression analysis, such as unstable coefficient estimates and inflated standard errors, making it difficult to interpret the individual effects of each variable on the dependent variable.\n",
    "\n",
    "To detect multicollinearity, you can use the following methods:\n",
    "\n",
    "1. *Correlation Matrix*: Calculate the correlation coefficients between all pairs of independent variables. If any correlation coefficient is close to 1 or -1, it indicates high multicollinearity.\n",
    "\n",
    "2. *Variance Inflation Factor (VIF)*: Compute the VIF for each independent variable. VIF measures how much the variance of an estimated regression coefficient is increased because of multicollinearity. High VIF values (typically above 10) suggest multicollinearity.\n",
    "\n",
    "To address multicollinearity, you can try the following techniques:\n",
    "\n",
    "1. *Feature Selection*: Remove one or more highly correlated independent variables from the model.\n",
    "\n",
    "2. *Combining Variables*: Instead of using highly correlated variables separately, create new variables that represent a combination of them.\n",
    "\n",
    "3. *Principal Component Analysis (PCA)*: Transform the original variables into a smaller set of uncorrelated variables, known as principal components, which can help mitigate multicollinearity.\n",
    "\n",
    "4. *Ridge Regression or Lasso Regression*: These regularization techniques penalize large coefficients, which can help stabilize them in the presence of multicollinearity.\n",
    "\n",
    "By detecting and addressing multicollinearity, you can improve the stability and interpretability of your multiple linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1dbca7",
   "metadata": {},
   "source": [
    "7) Polynomial regression is a type of regression analysis in which the relationship between the independent variable \\( x \\) and the dependent variable \\( y \\) is modeled as an \\( n \\)-th degree polynomial function\n",
    "\n",
    "Polynomial regression differs from linear regression in that it allows for non-linear relationships between the independent and dependent variables. While linear regression assumes a linear relationship between \\( x \\) and \\( y \\), polynomial regression can capture more complex, curved relationships.\n",
    "\n",
    "Linear regression models relationships using a straight line, while polynomial regression models relationships using a curved line, allowing for more flexibility in fitting the data. Polynomial regression can be useful when the relationship between the variables cannot be adequately captured by a linear model, such as in cases where the data exhibits curvature or non-linearity. However, polynomial regression can also be more prone to overfitting, especially with higher-degree polynomials, so it's important to strike a balance between model complexity and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1d09cb",
   "metadata": {},
   "source": [
    "8) Advantages of Polynomial Regression:\n",
    "1. *Flexibility*: Polynomial regression can model non-linear relationships between variables, allowing for more complex patterns to be captured in the data.\n",
    "2. *Better Fit*: When the relationship between the independent and dependent variables is curved or non-linear, polynomial regression can provide a better fit than linear regression.\n",
    "3. *Interpretability*: Depending on the degree of the polynomial, polynomial regression can still be relatively interpretable, especially for lower-degree polynomials.\n",
    "\n",
    "Disadvantages of Polynomial Regression:\n",
    "1. *Overfitting*: With higher-degree polynomials, polynomial regression is more prone to overfitting, where the model learns to fit the noise in the data rather than the underlying pattern.\n",
    "2. *Complexity*: Higher-degree polynomials can result in complex models that are difficult to interpret and may not generalize well to new data.\n",
    "3. *Extrapolation*: Polynomial regression can be unreliable when extrapolating beyond the range of the observed data, as the fitted curve may not accurately represent the true relationship outside this range.\n",
    "\n",
    "You might prefer to use polynomial regression in situations where:\n",
    "1. The relationship between the independent and dependent variables is clearly non-linear or exhibits curvature.\n",
    "2. Linear regression fails to adequately capture the patterns in the data.\n",
    "3. You are willing to trade off some interpretability for improved model fit.\n",
    "4. You have enough data to reliably estimate the parameters of the polynomial model and mitigate the risk of overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d511b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
